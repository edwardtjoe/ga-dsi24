{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782e70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup # Most common library for dealing with html\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b952d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two columns to focus on (From YouTube):\n",
    "# 1. What subreddit it comes from\n",
    "# 2. Title and Description of what post it comes from\n",
    "\n",
    "# df[['subreddit', 'selftext', 'title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d766b893",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 79\n",
      "2 163\n",
      "3 251\n",
      "4 328\n",
      "5 400\n",
      "6 476\n",
      "7 560\n",
      "8 634\n",
      "9 719\n",
      "10 807\n",
      "11 896\n",
      "12 980\n",
      "13 1060\n",
      "14 1138\n"
     ]
    }
   ],
   "source": [
    "personal_df = pd.DataFrame()\n",
    "insufficient_data = True\n",
    "epoch = 1632489845     # SG Time: Friday, September 24, 2021 9:24:05 PM\n",
    "loop_count = 0\n",
    "\n",
    "while insufficient_data:\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "        'subreddit': 'personalfinance',\n",
    "        'size': 100,\n",
    "        'before': epoch,\n",
    "    }\n",
    "    \n",
    "    old_epoch = epoch\n",
    "    \n",
    "    res = requests.get(url, params)\n",
    "    \n",
    "    # Sometimes, Error 502 occurs. To resolve, we request again (akin to refreshing page)\n",
    "    while res.status_code != 200:\n",
    "        time.sleep(0.5)\n",
    "        res = requests.get(url, params)\n",
    "        \n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    full_data = pd.DataFrame(posts)\n",
    "    \n",
    "    # Selecting the columns we want\n",
    "    selected_cols = full_data[['subreddit', 'title', 'selftext', 'created_utc']]\n",
    "    \n",
    "    # Dropping unusable selftext\n",
    "    selected_cols = selected_cols.dropna(subset=['selftext'])\n",
    "    selected_cols_clean = selected_cols.loc[(selected_cols['selftext'] != '[removed]')\n",
    "                                            & (selected_cols['selftext'] != '[deleted]')\n",
    "                                            & (selected_cols['selftext'] != '')]\n",
    "    \n",
    "    # Adding data to main dataframe\n",
    "    personal_df = pd.concat(objs=[personal_df, selected_cols_clean], axis=0)\n",
    "    \n",
    "    # Dropping duplicates within the same subreddit\n",
    "    personal_df.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "    \n",
    "    # Checking length of main dataframe\n",
    "    len(personal_df)\n",
    "        \n",
    "    # For checking only\n",
    "    loop_count += 1\n",
    "    print(loop_count, len(personal_df))\n",
    "    \n",
    "    # Setting new epoch for next loop as the earliest time created out of the 100 posts obtained\n",
    "    epoch = personal_df['created_utc'].iloc[-1]\n",
    "    \n",
    "    # Error tracking: If this is true, it will result in infinite loop\n",
    "    if old_epoch == epoch:\n",
    "        print('SAME OLDEST POST. ONE INFINITE LOOP')\n",
    "        insufficient_data = False\n",
    "    \n",
    "    if len(personal_df) >= 1100:\n",
    "        insufficient_data = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5893b520",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16\n",
      "2 28\n",
      "3 48\n",
      "4 62\n",
      "5 77\n",
      "6 87\n",
      "7 104\n",
      "8 119\n",
      "9 142\n",
      "10 154\n",
      "11 175\n",
      "12 186\n",
      "13 197\n",
      "14 211\n",
      "15 221\n",
      "16 237\n",
      "17 262\n",
      "18 272\n",
      "19 287\n",
      "20 293\n",
      "21 311\n",
      "22 322\n",
      "23 341\n",
      "24 357\n",
      "25 373\n",
      "26 400\n",
      "27 407\n",
      "28 421\n",
      "29 434\n",
      "30 450\n",
      "31 465\n",
      "32 481\n",
      "33 499\n",
      "34 518\n",
      "35 531\n",
      "36 547\n",
      "37 564\n",
      "38 581\n",
      "39 597\n",
      "40 615\n",
      "41 639\n",
      "42 662\n",
      "43 678\n",
      "44 696\n",
      "45 711\n",
      "46 727\n",
      "47 742\n",
      "48 756\n",
      "49 776\n",
      "50 790\n",
      "51 810\n",
      "52 821\n",
      "53 842\n",
      "54 862\n",
      "55 884\n",
      "56 902\n",
      "57 919\n",
      "58 935\n",
      "59 945\n",
      "60 961\n",
      "61 976\n",
      "62 999\n",
      "63 1019\n",
      "64 1035\n",
      "65 1053\n",
      "66 1070\n",
      "67 1093\n",
      "68 1113\n"
     ]
    }
   ],
   "source": [
    "investing_df = pd.DataFrame()\n",
    "insufficient_data = True\n",
    "epoch = 1632489845     # SG Time: Friday, September 24, 2021 9:24:05 PM\n",
    "loop_count = 0\n",
    "\n",
    "while insufficient_data:\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "        'subreddit': 'investing',\n",
    "        'size': 100,\n",
    "        'before': epoch,\n",
    "    }\n",
    "    \n",
    "    old_epoch = epoch\n",
    "    \n",
    "    res = requests.get(url, params)\n",
    "    \n",
    "    # Sometimes, Error 502 occurs. To resolve, we request again (akin to refreshing page)\n",
    "    while res.status_code != 200:\n",
    "        time.sleep(0.5)\n",
    "        res = requests.get(url, params)\n",
    "        \n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    full_data = pd.DataFrame(posts)\n",
    "    \n",
    "    # Selecting the columns we want\n",
    "    selected_cols = full_data[['subreddit', 'title', 'selftext', 'created_utc']]\n",
    "    \n",
    "    # Dropping unusable selftext\n",
    "    selected_cols = selected_cols.dropna(subset=['selftext'])\n",
    "    selected_cols_clean = selected_cols.loc[(selected_cols['selftext'] != '[removed]')\n",
    "                                            & (selected_cols['selftext'] != '[deleted]')\n",
    "                                            & (selected_cols['selftext'] != '')]\n",
    "    \n",
    "    # Adding data to main dataframe\n",
    "    investing_df = pd.concat(objs=[investing_df, selected_cols_clean], axis=0)\n",
    "    \n",
    "    # Dropping duplicates within the same subreddit\n",
    "    investing_df.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "    \n",
    "    # Checking length of main dataframe\n",
    "    len(investing_df)\n",
    "        \n",
    "    # For checking only\n",
    "    loop_count += 1\n",
    "    print(loop_count, len(investing_df))\n",
    "    \n",
    "    # Setting new epoch for next loop as the earliest time created out of the 100 posts obtained\n",
    "    epoch = investing_df['created_utc'].iloc[-1]\n",
    "    \n",
    "    # Error tracking: If this is true, it will result in infinite loop\n",
    "    if old_epoch == epoch:\n",
    "        print('SAME OLDEST POST. ONE INFINITE LOOP')\n",
    "        insufficient_data = False\n",
    "    \n",
    "    if len(investing_df) >= 1100:\n",
    "        insufficient_data = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a58a367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personalfinance    1138\n",
       "investing          1113\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat(objs=[investing_df, personal_df], axis=0)\n",
    "combined_df.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "combined_df.reset_index(inplace=True, drop=True)\n",
    "combined_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06696e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "p_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def regex_sucks(row):\n",
    "        \n",
    "    # Remove links\n",
    "    row['selftext'] = re.sub(\n",
    "        pattern=r'\\w+:\\/\\/[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', \n",
    "        repl='', \n",
    "        string=row['selftext'],\n",
    "        flags=re.M)\n",
    "    \n",
    "    # Remove subreddit name 'investing'\n",
    "    row['selftext'] = re.sub(\n",
    "        pattern=r'investing',\n",
    "        repl='',\n",
    "        string=row['selftext'],\n",
    "        flags=re.I)\n",
    "\n",
    "    # Remove subreddit name 'personalfinance'\n",
    "    row['selftext'] = re.sub(\n",
    "        pattern=r'personalfinance',\n",
    "        repl='',\n",
    "        string=row['selftext'],\n",
    "        flags=re.I)\n",
    "    \n",
    "    # Remove all digits\n",
    "    row['selftext'] = re.sub(\n",
    "        pattern=r'\\d+',\n",
    "        repl='',\n",
    "        string=row['selftext'])\n",
    "    \n",
    "    # Remove all special characters from selftext\n",
    "    row['selftext'] = re.sub(\n",
    "        pattern=r'\\W+',\n",
    "        repl=' ',\n",
    "        string=row['selftext'])\n",
    "    \n",
    "#    # Tokenize\n",
    "#     row['selftext'] = [tok for tok in tokenizer.tokenize(row['selftext'].lower())]\n",
    "    \n",
    "#    # Stemming\n",
    "#     row['selftext'] = [p_stemmer.stem(tok) for tok in row['selftext']]\n",
    "        \n",
    "#     # Drop english stop words\n",
    "#     row['selftext'] = [tok for tok in row['selftext'] if tok not in stopwords.words('english')]\n",
    "    \n",
    "    return row\n",
    "    \n",
    "clean_df = combined_df.apply(regex_sucks, axis=1)\n",
    "\n",
    "# Binarize column 'subreddit' where 'investing' == 1, 'personalfinance' == 0\n",
    "clean_df['subreddit'] = clean_df['subreddit'].map(lambda x: 1 if x == 'investing' else 0)\n",
    "clean_df.drop(columns=['created_utc'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f794c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toklem(x):\n",
    "    tok_lem = [p_stemmer.stem(token) for token in tokenizer.tokenize(x.lower()) if token not in stopwords.words('english')]\n",
    "    return ' '.join(tok_lem)\n",
    "clean_df['tok_stem'] = clean_df['selftext'].apply(toklem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokstem_text = []\n",
    "def tok_stem(row):\n",
    "    for token in tokenizer.tokenize(row['selftext'].lower()):\n",
    "        tokstem_text = []\n",
    "        if token not in stopwords.words('english'):\n",
    "            tokstem_text.append(p_stemmer.stem(token))\n",
    "        row['tok_stem_text'] = ' '.join(tokstem_text)\n",
    "    return row\n",
    "\n",
    "clean_df = clean_df.apply(tok_stem, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25326384",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokstem = []\n",
    "def tok_stem(text):\n",
    "    for token in tokenizer.tokenize(text.lower()):\n",
    "        if token not in stopwords.words('english'):\n",
    "            tokstem.append(p_stemmer.stem(token))\n",
    "    return ' '.join(tokstem)\n",
    "\n",
    "clean_df['tok_stem'] = clean_df['selftext'].apply(tok_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be2014c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>tok_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I have 4 begginer level questions about stocks...</td>\n",
       "      <td>First is about the bid ask spread if the bid i...</td>\n",
       "      <td>first bid ask spread bid dollar ask bidder sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should you follow insider transactions? - I an...</td>\n",
       "      <td>There is an old saying on Wall Street gt There...</td>\n",
       "      <td>old saying wall street gt many possible reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Value investment for the rebound?</td>\n",
       "      <td>I am a value investor a like buying cheap stuf...</td>\n",
       "      <td>value investor like buying cheap stuff bloombe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Daily General Discussion and spitballin thread...</td>\n",
       "      <td>Have a general question Want to offer some com...</td>\n",
       "      <td>general question want offer commentary market ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Daily Advice Thread - All basic help or advice...</td>\n",
       "      <td>If your question is I have what do I do or oth...</td>\n",
       "      <td>question advice personal situation question in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>0</td>\n",
       "      <td>An insurance company has been taking money out...</td>\n",
       "      <td>Backstory My parents aren t great with online ...</td>\n",
       "      <td>backstory parent great online banking noticed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>0</td>\n",
       "      <td>How do I prove lack of income?</td>\n",
       "      <td>I m an adult living with my parents who cover ...</td>\n",
       "      <td>adult living parent cover expense recently hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>0</td>\n",
       "      <td>Home equity loan for vacation rental and college?</td>\n",
       "      <td>Mid s married w kids Own home worth approx k o...</td>\n",
       "      <td>mid married w kid home worth approx k owe k re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249</th>\n",
       "      <td>0</td>\n",
       "      <td>Current refi rates?</td>\n",
       "      <td>Has anyone refinance recently in Georgia I am ...</td>\n",
       "      <td>anyone refinance recently georgia getting clos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2250</th>\n",
       "      <td>0</td>\n",
       "      <td>Low credit, repossession underway and short ti...</td>\n",
       "      <td>so i got thrown into the adult life at a very ...</td>\n",
       "      <td>got thrown adult life sudden moment everything...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2251 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit                                              title  \\\n",
       "0             1  I have 4 begginer level questions about stocks...   \n",
       "1             1  Should you follow insider transactions? - I an...   \n",
       "2             1                  Value investment for the rebound?   \n",
       "3             1  Daily General Discussion and spitballin thread...   \n",
       "4             1  Daily Advice Thread - All basic help or advice...   \n",
       "...         ...                                                ...   \n",
       "2246          0  An insurance company has been taking money out...   \n",
       "2247          0                     How do I prove lack of income?   \n",
       "2248          0  Home equity loan for vacation rental and college?   \n",
       "2249          0                                Current refi rates?   \n",
       "2250          0  Low credit, repossession underway and short ti...   \n",
       "\n",
       "                                               selftext  \\\n",
       "0     First is about the bid ask spread if the bid i...   \n",
       "1     There is an old saying on Wall Street gt There...   \n",
       "2     I am a value investor a like buying cheap stuf...   \n",
       "3     Have a general question Want to offer some com...   \n",
       "4     If your question is I have what do I do or oth...   \n",
       "...                                                 ...   \n",
       "2246  Backstory My parents aren t great with online ...   \n",
       "2247  I m an adult living with my parents who cover ...   \n",
       "2248  Mid s married w kids Own home worth approx k o...   \n",
       "2249  Has anyone refinance recently in Georgia I am ...   \n",
       "2250  so i got thrown into the adult life at a very ...   \n",
       "\n",
       "                                                tok_lem  \n",
       "0     first bid ask spread bid dollar ask bidder sti...  \n",
       "1     old saying wall street gt many possible reason...  \n",
       "2     value investor like buying cheap stuff bloombe...  \n",
       "3     general question want offer commentary market ...  \n",
       "4     question advice personal situation question in...  \n",
       "...                                                 ...  \n",
       "2246  backstory parent great online banking noticed ...  \n",
       "2247  adult living parent cover expense recently hos...  \n",
       "2248  mid married w kid home worth approx k owe k re...  \n",
       "2249  anyone refinance recently georgia getting clos...  \n",
       "2250  got thrown adult life sudden moment everything...  \n",
       "\n",
       "[2251 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c162105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First is about the bid ask spread if the bid is dollars and the ask is the bidder will still pay dollars and the ask will receive correct So what is the point of the bidder doing a bid if he is going to pay the full amount Does he choose how much money goes to spread and how much to the seller The second question is how is it beneficial in the long term for a company to give shares if the profits are going to shareholders The third question is about crypto why would crypto be in demand if we can just use regular currency The fourth question is about stock dividends if a company pays you in stocks instead of money how would you make money from that '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['selftext'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155b9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_clean_df = clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f504ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.505553\n",
       "1    0.494447\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline: 50.6% from personalfinance\n",
    "\n",
    "clean_df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b952b",
   "metadata": {},
   "source": [
    "# Step 1: Tokenize --> DONE\n",
    "## 1. Regex\n",
    "# Step 2: Porter Stem --> DONE\n",
    "# Step 3: Drop Stop Words --> DONE\n",
    "# Step 4: CVEC\n",
    "## 1. Do some EDA\n",
    "# Step 5: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db3b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer()\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "X = clean_df[['title', 'selftext']]\n",
    "y = clean_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                  random_state=42,\n",
    "                                                  stratify=y\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b58bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corp = cvec.fit_transform(X_train)\n",
    "X_test_corp = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd01cdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train_corp.todense(), \n",
    "                       columns=cvec.get_feature_names())\n",
    "\n",
    "X_train_df.sum().sort_values(ascending=False).head(20).plot(kind='barh')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
