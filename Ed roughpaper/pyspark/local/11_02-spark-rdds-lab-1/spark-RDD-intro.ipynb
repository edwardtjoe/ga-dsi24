{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Spark RDDs\n",
    "\n",
    "*Authors: Adapted and modified from Dave Yerrington (SF) by Christoph Rahmede (LDN)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lab Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Transformations-:-transforming-an-RDD-into-another\" data-toc-modified-id=\"Transformations-:-transforming-an-RDD-into-another-1\">Transformations : transforming an RDD into another</a></span></li><li><span><a href=\"#Actions-:-turning-your-RDD-into-something-else-(local-object)\" data-toc-modified-id=\"Actions-:-turning-your-RDD-into-something-else-(local-object)-2\">Actions : turning your RDD into something else (local object)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Applying-transformations-and-chaining-them\" data-toc-modified-id=\"Applying-transformations-and-chaining-them-2.1\">Applying transformations and chaining them</a></span></li></ul></li><li><span><a href=\"#Load-data-with-python\" data-toc-modified-id=\"Load-data-with-python-3\">Load data with python</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#sc.textFile:-Load-data/sales.txt-into-an-RDD\" data-toc-modified-id=\"sc.textFile:-Load-data/sales.txt-into-an-RDD-3.0.1\"><code>sc.textFile</code>: Load <code>data/sales.txt</code> into an RDD</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.0.2\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#Actions-that-return-portions-of-an-RDD\" data-toc-modified-id=\"Actions-that-return-portions-of-an-RDD-3.1\">Actions that return portions of an RDD</a></span><ul class=\"toc-item\"><li><span><a href=\"#.collect()-:-return-the-full-content-of-the-RDD-to-&quot;python-space&quot;\" data-toc-modified-id=\".collect()-:-return-the-full-content-of-the-RDD-to-&quot;python-space&quot;-3.1.1\"><code>.collect()</code> : return the <em>full</em> content of the RDD to \"python space\"</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.1.2\">Solution (double click)</a></span></li><li><span><a href=\"#.take(n)-:-Return-n-lines-of-an-RDD\" data-toc-modified-id=\".take(n)-:-Return-n-lines-of-an-RDD-3.1.3\"><code>.take(n)</code> : Return n lines of an RDD</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.1.4\">Solution (double click)</a></span></li><li><span><a href=\"#.first()-:-Return-the-first-line-of-the-RDD\" data-toc-modified-id=\".first()-:-Return-the-first-line-of-the-RDD-3.1.5\"><code>.first()</code> : Return the first line of the RDD</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.1.6\">Solution (double click)</a></span></li><li><span><a href=\"#.map(func)-:-Apply-a-lambda-function-to-split-each-row-into-a-list-and-to-produce-a-list-of-lists\" data-toc-modified-id=\".map(func)-:-Apply-a-lambda-function-to-split-each-row-into-a-list-and-to-produce-a-list-of-lists-3.1.7\"><code>.map(func)</code> : Apply a lambda-function to split each row into a list and to produce a list of lists</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.1.8\">Solution (double click)</a></span></li><li><span><a href=\"#Do-the-same-but-using-.flatMap(func)-.-What-is-the-difference?\" data-toc-modified-id=\"Do-the-same-but-using-.flatMap(func)-.-What-is-the-difference?-3.1.9\">Do the same but using <code>.flatMap(func)</code> . What is the difference?</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.1.10\">Solution (double click)</a></span></li><li><span><a href=\"#.filter(func):-filters-an-RDD-using-a-function-that-returns-boolean-values\" data-toc-modified-id=\".filter(func):-filters-an-RDD-using-a-function-that-returns-boolean-values-3.1.11\"><code>.filter(func)</code>: filters an RDD using a function that returns boolean values</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.1.12\">Solution (double click)</a></span></li><li><span><a href=\"#Apply-a-custom-function-to-the-filtered-RDD-resulting-from-the-last-step,-so-that-each-entry-has-the-appropriate-data-format.\" data-toc-modified-id=\"Apply-a-custom-function-to-the-filtered-RDD-resulting-from-the-last-step,-so-that-each-entry-has-the-appropriate-data-format.-3.1.13\">Apply a custom function to the filtered RDD resulting from the last step, so that each entry has the appropriate data format.</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-3.1.14\">Solution (double click)</a></span></li><li><span><a href=\"#Now,-let's-see-the-canonical-way-to-write-that-in-Python...\" data-toc-modified-id=\"Now,-let's-see-the-canonical-way-to-write-that-in-Python...-3.1.15\">Now, let's see the canonical way to write that in Python...</a></span></li></ul></li></ul></li><li><span><a href=\"#-Practical-tips\" data-toc-modified-id=\"-Practical-tips-4\"><i class=\"fa fa-thumbs-up\"></i> Practical tips</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sampling-from-enormous-datasets\" data-toc-modified-id=\"Sampling-from-enormous-datasets-4.1\">Sampling from enormous datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#.sample(withReplacement,-fraction,-seed):-sampling-an-RDD-!!\" data-toc-modified-id=\".sample(withReplacement,-fraction,-seed):-sampling-an-RDD-!!-4.1.1\"><code>.sample(withReplacement, fraction, seed)</code>: sampling an RDD !!</a></span></li><li><span><a href=\"#.distinct():-obtaining-distinct-rows\" data-toc-modified-id=\".distinct():-obtaining-distinct-rows-4.1.2\"><code>.distinct()</code>: obtaining distinct rows</a></span></li></ul></li><li><span><a href=\"#Methods-with-a-<key,-value>-paradigm\" data-toc-modified-id=\"Methods-with-a-<key,-value>-paradigm-4.2\">Methods with a <code>&lt;key, value&gt;</code> paradigm</a></span><ul class=\"toc-item\"><li><span><a href=\"#.keys():-returns-the-keys-of-an-RDD-made-of-<k,v>-pairs\" data-toc-modified-id=\".keys():-returns-the-keys-of-an-RDD-made-of-<k,v>-pairs-4.2.1\"><code>.keys()</code>: returns the keys of an RDD made of <code>&lt;k,v&gt;</code> pairs</a></span></li><li><span><a href=\"#.values():-returns-the-values-of-an-RDD-made-of-<key,-value>-pairs\" data-toc-modified-id=\".values():-returns-the-values-of-an-RDD-made-of-<key,-value>-pairs-4.2.2\"><code>.values()</code>: returns the values of an RDD made of <code>&lt;key, value&gt;</code> pairs</a></span></li><li><span><a href=\"#rddA.join(rddB):-join-another-RDD\" data-toc-modified-id=\"rddA.join(rddB):-join-another-RDD-4.2.3\"><code>rddA.join(rddB)</code>: join another RDD</a></span></li><li><span><a href=\"#.reduceByKey(func):-reduce-values-by-their-key-by-applying-func\" data-toc-modified-id=\".reduceByKey(func):-reduce-values-by-their-key-by-applying-func-4.2.4\"><code>.reduceByKey(func)</code>: reduce <code>values</code> by their <code>key</code> by applying func</a></span></li><li><span><a href=\"#.groupByKey(func):-reduce-values-by-their-keys-by-applying-a-function\" data-toc-modified-id=\".groupByKey(func):-reduce-values-by-their-keys-by-applying-a-function-4.2.5\"><code>.groupByKey(func)</code>: reduce <code>values</code> by their <code>keys</code> by applying a function</a></span></li></ul></li><li><span><a href=\"#Sorting-methods\" data-toc-modified-id=\"Sorting-methods-4.3\">Sorting methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#.sortBy(keyfunc):-sorting-by-the-value-of-a-function-on-rows\" data-toc-modified-id=\".sortBy(keyfunc):-sorting-by-the-value-of-a-function-on-rows-4.3.1\"><code>.sortBy(keyfunc)</code>: sorting by the value of a function on rows</a></span></li><li><span><a href=\"#.sortByKey():-sorting-by-key-on-a-<k,v>-RDD\" data-toc-modified-id=\".sortByKey():-sorting-by-key-on-a-<k,v>-RDD-4.3.2\"><code>.sortByKey()</code>: sorting by key on a <code>&lt;k,v&gt;</code> RDD</a></span></li></ul></li><li><span><a href=\"#Actions-that-compute-some-statistics\" data-toc-modified-id=\"Actions-that-compute-some-statistics-4.4\">Actions that compute some statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#.count()-:-count-the-number-of-lines\" data-toc-modified-id=\".count()-:-count-the-number-of-lines-4.4.1\"><code>.count()</code> : count the number of lines</a></span></li><li><span><a href=\"#.sum():-summing-every-line-in-an-RDD\" data-toc-modified-id=\".sum():-summing-every-line-in-an-RDD-4.4.2\"><code>.sum()</code>: summing every line in an RDD</a></span></li><li><span><a href=\"#.mean():-averaging-every-line-in-an-RDD\" data-toc-modified-id=\".mean():-averaging-every-line-in-an-RDD-4.4.3\"><code>.mean()</code>: averaging every line in an RDD</a></span></li><li><span><a href=\"#.stdev():\" data-toc-modified-id=\".stdev():-4.4.4\"><code>.stdev()</code>:</a></span></li><li><span><a href=\"#.stats():\" data-toc-modified-id=\".stats():-4.4.5\">.stats():</a></span></li></ul></li></ul></li><li><span><a href=\"#Let's-design-chains-of-transformations\" data-toc-modified-id=\"Let's-design-chains-of-transformations-5\">Let's design chains of transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Computing-sales-per-state\" data-toc-modified-id=\"Computing-sales-per-state-5.1\">Computing sales per state</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-RDD\" data-toc-modified-id=\"Input-RDD-5.1.1\">Input RDD</a></span></li><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-5.1.2\">Task</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-5.1.3\">Code</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-5.1.4\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#Word-count-(again)\" data-toc-modified-id=\"Word-count-(again)-5.2\">Word count (again)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-RDD\" data-toc-modified-id=\"Input-RDD-5.2.1\">Input RDD</a></span></li><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-5.2.2\">Task</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-5.2.3\">Code</a></span></li><li><span><a href=\"#Solution-(double-click)\" data-toc-modified-id=\"Solution-(double-click)-5.2.4\">Solution (double click)</a></span></li></ul></li><li><span><a href=\"#Find-the-date-on-which-AAPL's-stock-closing-price-was-the-highest\" data-toc-modified-id=\"Find-the-date-on-which-AAPL's-stock-closing-price-was-the-highest-5.3\">Find the date on which AAPL's stock closing price was the highest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-RDD\" data-toc-modified-id=\"Input-RDD-5.3.1\">Input RDD</a></span></li><li><span><a href=\"#Task\" data-toc-modified-id=\"Task-5.3.2\">Task</a></span></li><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-5.3.3\">Code</a></span></li><li><span><a href=\"#Solution\" data-toc-modified-id=\"Solution-5.3.4\">Solution</a></span></li></ul></li></ul></li><li><span><a href=\"#What's-the-in-memory-you-are-talking-about-?\" data-toc-modified-id=\"What's-the-in-memory-you-are-talking-about-?-6\">What's the in-memory you are talking about ?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Caching-/-Persistency\" data-toc-modified-id=\"Caching-/-Persistency-6.1\">Caching / Persistency</a></span></li><li><span><a href=\"#Caching\" data-toc-modified-id=\"Caching-6.2\">Caching</a></span></li><li><span><a href=\"#Persist\" data-toc-modified-id=\"Persist-6.3\">Persist</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations : transforming an RDD into another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |-|\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Group the values for each key in the RDD into a single sequence. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions : turning your RDD into something else (local object)\n",
    "\n",
    "Actions are specific methods of an RDD object, they are usually designed to transform an RDD into something else (e.g. a python object or a statistic).\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | Type | Description |\n",
    "| - | - | - |\n",
    "|[`.reduce()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce)| action | Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.|\n",
    "| [`.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) | action | Return a list that contains all of the elements in this RDD. Note that this method should only be used if the resulting array is expected to be small as all the data is loaded into the driver’s memory. |\n",
    "| [`.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) | action | Return the number of elements in this RDD. |\n",
    "| [`.take(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) | action | Take the first `n` elements of the RDD. |\n",
    "| [`.top(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) | action | Get the top `n` elements from a RDD. It returns the list sorted in descending order. |\n",
    "| [`.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first) | action | Return the first element in an RDD. |\n",
    "| [`.sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sum) | action | Add up the elements in this RDD. |\n",
    "| [`.mean()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean) | action | Compute the mean of this RDD’s elements. |\n",
    "| [`.stdev()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.stdev) | action | Compute the standard deviation of this RDD’s elements. |\n",
    "| [`.countByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)|action| Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying transformations and chaining them\n",
    "\n",
    "Recall the spark flow:\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "We'll proceed along the usual spark flow (see above):\n",
    "1. Create the environment to run spark from python\n",
    "2. Extract RDDs from files\n",
    "3. Run some transformations\n",
    "4. Execute actions to obtain values (local objects in python)\n",
    "\n",
    "Each transformation is a method of an RDD and returns another RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ID    Date           Store   State  Product    Amount\n",
      "101    11/13/2014     100     WA     331        300.00\n",
      "104    11/18/2014     700     OR     329        450.00\n",
      "102    11/15/2014     203     CA     321        200.00\n",
      "106    11/19/2014     202     CA     331        330.00\n",
      "103    11/17/2014     101     WA     373        750.00\n",
      "105    11/19/2014     202     CA     321        200.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/sales.txt', 'r') as fin:\n",
    "    print(fin.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall: Input functions, reading RDDs from files, are functions of the SparkContext.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "sc = ps.SparkContext('local[2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile`: Load `data/sales.txt` into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rdd1 = sc.textFile('data/sales.txt')\n",
    "<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions that return portions of an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.collect()` : return the *full* content of the RDD to \"python space\"\n",
    "\n",
    "Returns the rows of an RDD as a list. Can be a bad idea if your RDD is gigantic because `.collect()` will return everything and put it in memory for python to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rdd1.collect()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.take(n)` : Return n lines of an RDD\n",
    "\n",
    "Returns `n` rows of an RDD as a list. These `n` are not randomly selected. They are Spark's own internal mechanism for obtaining the lines that can be collected first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rdd1.take(2)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.first()` : Return the first line of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rdd1.first()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.map(func)` : Apply a lambda-function to split each row into a list and to produce a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rdd2 = rdd1.map(lambda row: row.split())\n",
    "rdd2.collect()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the same but using `.flatMap(func)` . What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "out_rdd = rdd2.flatMap(lambda x: (x[2], x[3]))\n",
    "out_rdd.collect()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.filter(func)`: filters an RDD using a function that returns boolean values\n",
    "\n",
    "Filter your RDD so that no row containing a hash appears in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "rdd3 = rdd2.filter(lambda row: not row[0].startswith('#'))\n",
    "rdd3.collect()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply a custom function to the filtered RDD resulting from the last step, so that each entry has the appropriate data format.\n",
    "\n",
    "The function should look at each row, turn the first entry into an integer, leave the second as it is (a string), turn the third entry into an integer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "def casting_function(X):\n",
    "    return int(X[0]), X[1], int(X[2]), X[3], int(X[4]), float(X[5])\n",
    "rdd4 = rdd3.map(casting_function)\n",
    "rdd4.collect()\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's see the canonical way to write that in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ID    Date           Store   State  Product    Amount',\n",
       " '101    11/13/2014     100     WA     331        300.00',\n",
       " '104    11/18/2014     700     OR     329        450.00',\n",
       " '102    11/15/2014     203     CA     321        200.00',\n",
       " '106    11/19/2014     202     CA     331        330.00',\n",
       " '103    11/17/2014     101     WA     373        750.00',\n",
       " '105    11/19/2014     202     CA     321        200.00']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt')\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#ID', 'Date', 'Store', 'State', 'Product', 'Amount'],\n",
       " ['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda rowstr: rowstr.split())   # <= JUST ADDED THIS HERE\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', '11/13/2014', '100', 'WA', '331', '300.00'],\n",
       " ['104', '11/18/2014', '700', 'OR', '329', '450.00'],\n",
       " ['102', '11/15/2014', '203', 'CA', '321', '200.00'],\n",
       " ['106', '11/19/2014', '202', 'CA', '331', '330.00'],\n",
       " ['103', '11/17/2014', '101', 'WA', '373', '750.00'],\n",
       " ['105', '11/19/2014', '202', 'CA', '321', '200.00']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda rowstr: rowstr.split()) \\\n",
    "              .filter(lambda row: not row[0].startswith('#'))    # <= JUST ADDED THIS HERE\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casting_function(X):\n",
    "    return int(X[0]), X[1], int(X[2]), X[3], int(X[4]), float(X[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda rowstr: rowstr.split()) \\\n",
    "              .filter(lambda row: not row[0].startswith('#')) \\\n",
    "              .map(casting_function)   # <= JUST ADDED THIS HERE\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda x: x.split()) \\\n",
    "              .filter(lambda x: not x[0].startswith('#')) \\\n",
    "              .map(casting_function)\n",
    "\n",
    "sales_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-thumbs-up\" aria-hidden=\"true\"></i> Practical tips\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from enormous datasets\n",
    "Undoubtably, you may have the need to examine a larger dataset.  A common operation is to take a sample.  To approximate the characteristics of your global distribution, you should try to adjust the size that best matches the metrics of central tendency or consider doing a power analysis to determine sample sizing.\n",
    "\n",
    "> Size of your sample generally depends on your application be it A/B testing, EDA, Machine Learning, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sample(withReplacement, fraction, seed)`: sampling an RDD !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling an rdd\n",
    "out_rdd = sales_rdd.sample(True, 0.4)\n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rdd.takeSample(True, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.distinct()`: obtaining distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: \n",
      "[(101, '11/13/2014', 100, 'WA', 331, 300.0), (104, '11/18/2014', 700, 'OR', 329, 450.0), (102, '11/15/2014', 203, 'CA', 321, 200.0), (106, '11/19/2014', 202, 'CA', 331, 330.0), (103, '11/17/2014', 101, 'WA', 373, 750.0), (105, '11/19/2014', 202, 'CA', 321, 200.0)]\n",
      "\n",
      "after: \n",
      "['CA', 'WA', 'OR']\n"
     ]
    }
   ],
   "source": [
    "# obtaining distinct values of the \"state\" column of rdd_sales\n",
    "out_rdd = sales_rdd.map(lambda x: x[3]) \\\n",
    "                   .distinct()\n",
    "\n",
    "# print out the original rdd\n",
    "print(\"before: \\n{}\".format(sales_rdd.collect()))\n",
    "\n",
    "# print out the new rdd generated\n",
    "print(\"\\n\" + \"after: \\n{}\".format(out_rdd.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods with a `<key, value>` paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WA', 300.0),\n",
       " ('OR', 450.0),\n",
       " ('CA', 200.0),\n",
       " ('CA', 330.0),\n",
       " ('WA', 750.0),\n",
       " ('CA', 200.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a subset of the rdd to be used as key-value pairs\n",
    "sales_subset = sales_rdd.map(lambda x: (x[3], x[-1]))\n",
    "sales_subset.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.keys()`: returns the keys of an RDD made of `<k,v>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WA', 'OR', 'CA', 'CA', 'WA', 'CA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.values()`: returns the values of an RDD made of `<key, value>` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300.0, 450.0, 200.0, 330.0, 750.0, 200.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `rddA.join(rddB)`: join another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', (200.0, 'Hot')),\n",
       " ('CA', (330.0, 'Hot')),\n",
       " ('CA', (200.0, 'Hot')),\n",
       " ('WA', (300.0, 'Boring')),\n",
       " ('WA', (750.0, 'Boring')),\n",
       " ('OR', (450.0, 'Cold'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an adhoc list for each state\n",
    "managers_array = [['CA', 'Hot'],\n",
    "                  ['OR', 'Cold'],\n",
    "                  ['WA', 'Boring'],\n",
    "                  ['TX', 'Gun']]\n",
    "\n",
    "# reading the array/list using SparkContext\n",
    "managers_rdd = sc.parallelize(managers_array)\n",
    "\n",
    "# to output the content in python (use with great care)\n",
    "sales_subset.join(managers_rdd).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.reduceByKey(func)`: reduce `values` by their `key` by applying func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 730.0), ('WA', 1050.0), ('OR', 450.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.groupByKey(func)`: reduce `values` by their `keys` by applying a function \n",
    "This can use any function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', 243.33333333333334), ('WA', 525.0), ('OR', 450.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mean_func(iterator):\n",
    "    total, count = 0, 0\n",
    "    for x in iterator:\n",
    "        total += x\n",
    "        count += 1\n",
    "    return total / count\n",
    "\n",
    "\n",
    "sales_subset.groupByKey() \\\n",
    "    .map(lambda x: (x[0], mean_func(x[1]))) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CA', <pyspark.resultiterable.ResultIterable at 0x10d90f710>),\n",
       " ('WA', <pyspark.resultiterable.ResultIterable at 0x10d90f6d0>),\n",
       " ('OR', <pyspark.resultiterable.ResultIterable at 0x10d90f750>)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortBy(keyfunc)`: sorting by the value of a function on rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd.sortBy(lambda x: x[3], ascending=True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sortByKey()`: sorting by key on a `<k,v>` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WA', 300.0),\n",
       " ('WA', 750.0),\n",
       " ('OR', 450.0),\n",
       " ('CA', 200.0),\n",
       " ('CA', 330.0),\n",
       " ('CA', 200.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting k,v pairs by key\n",
    "sales_subset.sortByKey(ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions that compute some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.count()` : count the number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.sum()`: summing every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.values().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.mean()`: averaging every line in an RDD\n",
    "\n",
    "(The RDD needs to be containing summable values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371.6666666666667"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.values().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.stdev()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189.33362676033602"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.values().stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .stats():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 6, mean: 371.6666666666667, stdev: 189.33362676033602, max: 750.0, min: 200.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_subset.values().stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's design chains of transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing sales per state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casting_function(x):\n",
    "    (_id, date, store, state, product, amount) = x\n",
    "    return((int(_id), date, int(store), state, int(product), float(amount)))\n",
    "\n",
    "\n",
    "rdd_sales = sc.textFile('data/sales.txt') \\\n",
    "              .map(lambda x: x.split()) \\\n",
    "              .filter(lambda x: not x[0].startswith('#')) \\\n",
    "              .map(casting_function)\n",
    "\n",
    "rdd_sales.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "You want to obtain a sorted RDD of the states in which you have most sales done (amount).\n",
    "\n",
    "What transformations do you need to apply ?\n",
    "If you had to draw a workflow of the transformations to apply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, '11/13/2014', 100, 'WA', 331, 300.0),\n",
       " (104, '11/18/2014', 700, 'OR', 329, 450.0),\n",
       " (102, '11/15/2014', 203, 'CA', 321, 200.0),\n",
       " (106, '11/19/2014', 202, 'CA', 331, 330.0),\n",
       " (103, '11/17/2014', 101, 'WA', 373, 750.0),\n",
       " (105, '11/19/2014', 202, 'CA', 321, 200.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rdd = sales_rdd  # apply transformation here...\n",
    "\n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\"><br/>\n",
    "out_rdd = rdd_sales.map(lambda x: (x[3], x[5])) \\\n",
    "                   .reduceByKey(lambda x, y: x + y) \\\n",
    "                   .sortBy(lambda x: x[1], ascending=False)\n",
    "<br/>\n",
    "out_rdd.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "another line\n",
      "yet another line\n",
      "yet another another line\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the content of the file in stdout\n",
    "with open('data/input.txt', 'r') as fin:\n",
    "    print(fin.read())\n",
    "\n",
    "# reading the file using SparkContext\n",
    "\n",
    "rdd = sc.textFile('data/input.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "What transformations do you need to apply in order to do the word count?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world', 'another line', 'yet another line', 'yet another another line']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rdd = rdd  # apply transformation here...\n",
    "\n",
    "# collect the result\n",
    "out_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution (double click)\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "out_rdd = rdd.flatMap(lambda x : x.split()) \\\n",
    "             .map(lambda x: (x, 1)) \\\n",
    "             .reduceByKey(lambda x, y: x + y)\n",
    "<br/>\n",
    "out_rdd.collect()<br/>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the date on which AAPL's stock closing price was the highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines in file: 254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " '2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " '2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " '2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appl_raw_rdd = sc.textFile('data/aapl.csv')\n",
    "\n",
    "print(\"lines in file: {}\".format(appl_raw_rdd.count()))\n",
    "\n",
    "appl_raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Now, design a pipeline that would :\n",
    "1. filter out headers\n",
    "2. split each line based on comma\n",
    "3. keep only fields for Date (col 0) and Close (col 4)\n",
    "4. order by Close in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,Volume,Adj Close',\n",
       " '2016-10-25,117.949997,118.360001,117.309998,118.25,39190300,118.25',\n",
       " '2016-10-24,117.099998,117.739998,117.00,117.650002,23538700,117.650002',\n",
       " '2016-10-21,116.809998,116.910004,116.279999,116.599998,23192700,116.599998',\n",
       " '2016-10-20,116.860001,117.379997,116.330002,117.059998,24125800,117.059998']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_rdd = appl_raw_rdd  # apply transformation here...\n",
    "\n",
    "out_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "<span style=\"color:white;font-family:'Courier New'\">\n",
    "out_rdd = appl_raw_rdd.filter(lambda x: not x.startswith(\"Date\")) \\\n",
    "                      .map(lambda x: x.split(\",\")) \\\n",
    "                      .map(lambda x: (x[0], float(x[4]))) \\\n",
    "                      .sortBy(lambda x: x[1], ascending=False)\n",
    "<br/>\n",
    "out_rdd.collect()<br/>\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the in-memory you are talking about ?\n",
    "\n",
    "Recall:\n",
    "- Spark runs in memory and on disk\n",
    "- Can be up to 100x faster than Hadoop MapReduce in memory, and 10x faster on disk.\n",
    "- Spark keeps everything in memory when possible, uses lots of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching / Persistency\n",
    "\n",
    "- The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "- If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    "- This is especially useful if you have to run the same computation over and over again on one RDD like in common machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Consider the following job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 5*10**5\n",
    "num_list = [random.random() for i in range(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 ms ± 50.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[92] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 ms ± 59.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.4 ms ± 9.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "- Calling cache() flips a flag on the RDD.\n",
    "- The data is not cached until an action is called.\n",
    "- You can uncache an RDD using unpersist()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist\n",
    "\n",
    "- Persist RDD to disk instead of caching it in memory.\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "| Level\t| Meaning |\n",
    "| - | - |\n",
    "| MEMORY_ONLY\t| Same as cache() |\n",
    "| MEMORY_AND_DISK\t| Cache in memory then overflow to disk |\n",
    "| MEMORY_AND_DISK_SER\t| Like above; in cache keep objects serialized instead of live |\n",
    "| DISK_ONLY\t| Cache to disk not to memory |\n",
    "\n",
    "See [here](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=persist#pyspark.StorageLevel) for storage levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[92] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lab Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "579px",
    "left": "561.111px",
    "right": "20px",
    "top": "120px",
    "width": "337px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
