{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Practice Spark Lab\n",
    "\n",
    "*Authors: Christoph Rahmede (LDN)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, we will use Spark to dig into the Bay Area Bike Share data.**\n",
    "\n",
    "Our goal is to calculate the average number of trips per hour, using the Caltrain Station as starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ps.SparkContext('local[4]')\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = ps.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = spark.read.csv(\n",
    "    path=\"./data/201508_trip_data.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "trips.registerTempTable(\"tripsSql_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "|Trip ID|Duration|     Start Date|       Start Station|Start Terminal|       End Date|         End Station|End Terminal|Bike #|Subscriber Type|Zip Code|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "| 913460|     765|8/31/2015 23:26|Harry Bridges Pla...|            50|8/31/2015 23:39|San Francisco Cal...|          70|   288|     Subscriber|    2139|\n",
      "| 913459|    1036|8/31/2015 23:11|San Antonio Shopp...|            31|8/31/2015 23:28|Mountain View Cit...|          27|    35|     Subscriber|   95032|\n",
      "| 913455|     307|8/31/2015 23:13|      Post at Kearny|            47|8/31/2015 23:18|   2nd at South Park|          64|   468|     Subscriber|   94107|\n",
      "| 913454|     409|8/31/2015 23:10|  San Jose City Hall|            10|8/31/2015 23:17| San Salvador at 1st|           8|    68|     Subscriber|   95113|\n",
      "| 913453|     789|8/31/2015 23:09|Embarcadero at Fo...|            51|8/31/2015 23:22|Embarcadero at Sa...|          60|   487|       Customer|    9069|\n",
      "| 913452|     293|8/31/2015 23:07|Yerba Buena Cente...|            68|8/31/2015 23:12|San Francisco Cal...|          70|   538|     Subscriber|   94118|\n",
      "| 913451|     896|8/31/2015 23:07|Embarcadero at Fo...|            51|8/31/2015 23:22|Embarcadero at Sa...|          60|   363|       Customer|   92562|\n",
      "| 913450|     255|8/31/2015 22:16|Embarcadero at Sa...|            60|8/31/2015 22:20|   Steuart at Market|          74|   470|     Subscriber|   94111|\n",
      "| 913449|     126|8/31/2015 22:12|     Beale at Market|            56|8/31/2015 22:15|Temporary Transba...|          55|   439|     Subscriber|   94130|\n",
      "| 913448|     932|8/31/2015 21:57|      Post at Kearny|            47|8/31/2015 22:12|South Van Ness at...|          66|   472|     Subscriber|   94702|\n",
      "+-------+--------+---------------+--------------------+--------------+---------------+--------------------+------------+------+---------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamps\n",
    "\n",
    "You can use the following functions to cast into a timestamp and to extract parts of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, to_date, to_timestamp, year, month, dayofweek, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trips.withColumn('time', to_timestamp('Start Date', format='MM/dd/yyyy HH:mm'))\n",
    "df = df.withColumn('hour', hour('time'))\n",
    "df = df.withColumn('day', to_date('time'))\n",
    "df = df.withColumn('month', month('time'))\n",
    "df = df.withColumn('weekday', dayofweek('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+----+----------+-----+-------+\n",
      "|     Start Date|               time|hour|       day|month|weekday|\n",
      "+---------------+-------------------+----+----------+-----+-------+\n",
      "|8/31/2015 23:26|2015-08-31 23:26:00|  23|2015-08-31|    8|      2|\n",
      "|8/31/2015 23:11|2015-08-31 23:11:00|  23|2015-08-31|    8|      2|\n",
      "|8/31/2015 23:13|2015-08-31 23:13:00|  23|2015-08-31|    8|      2|\n",
      "|8/31/2015 23:10|2015-08-31 23:10:00|  23|2015-08-31|    8|      2|\n",
      "|8/31/2015 23:09|2015-08-31 23:09:00|  23|2015-08-31|    8|      2|\n",
      "|8/31/2015 23:07|2015-08-31 23:07:00|  23|2015-08-31|    8|      2|\n",
      "|8/31/2015 23:07|2015-08-31 23:07:00|  23|2015-08-31|    8|      2|\n",
      "|8/31/2015 22:16|2015-08-31 22:16:00|  22|2015-08-31|    8|      2|\n",
      "|8/31/2015 22:12|2015-08-31 22:12:00|  22|2015-08-31|    8|      2|\n",
      "|8/31/2015 21:57|2015-08-31 21:57:00|  21|2015-08-31|    8|      2|\n",
      "+---------------+-------------------+----+----------+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Start Date', 'time', 'hour', 'day', 'month', 'weekday').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our datetime parsing has not been perfect. We can check for missing values. We should really try to cure this. Here let's just drop the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>Start Station</th>\n",
       "      <th>Start Terminal</th>\n",
       "      <th>End Date</th>\n",
       "      <th>End Station</th>\n",
       "      <th>End Terminal</th>\n",
       "      <th>Bike #</th>\n",
       "      <th>Subscriber Type</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>702587</td>\n",
       "      <td>699</td>\n",
       "      <td>3/29/2015 1:43</td>\n",
       "      <td>San Jose Diridon Caltrain Station</td>\n",
       "      <td>2</td>\n",
       "      <td>3/29/2015 1:55</td>\n",
       "      <td>Japantown</td>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>95112</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>702585</td>\n",
       "      <td>226</td>\n",
       "      <td>3/29/2015 1:30</td>\n",
       "      <td>Grant Avenue at Columbus Avenue</td>\n",
       "      <td>73</td>\n",
       "      <td>3/29/2015 1:34</td>\n",
       "      <td>Broadway St at Battery St</td>\n",
       "      <td>82</td>\n",
       "      <td>563</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>94114</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Trip ID  Duration      Start Date                      Start Station  \\\n",
       "0   702587       699  3/29/2015 1:43  San Jose Diridon Caltrain Station   \n",
       "1   702585       226  3/29/2015 1:30    Grant Avenue at Columbus Avenue   \n",
       "\n",
       "   Start Terminal        End Date                End Station  End Terminal  \\\n",
       "0               2  3/29/2015 1:55                  Japantown             9   \n",
       "1              73  3/29/2015 1:34  Broadway St at Battery St            82   \n",
       "\n",
       "   Bike # Subscriber Type Zip Code time  hour   day  month  weekday  \n",
       "0      79      Subscriber    95112  NaT   NaN  None    NaN      NaN  \n",
       "1     563      Subscriber    94114  NaT   NaN  None    NaN      NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.where(col('day').isNull()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trip ID: int, Duration: int, Start Date: string, Start Station: string, Start Terminal: int, End Date: string, End Station: string, End Terminal: int, Bike #: int, Subscriber Type: string, Zip Code: string, time: timestamp, hour: int, day: date, month: int, weekday: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(col('day').isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trip ID: int, Duration: int, Start Date: string, Start Station: string, Start Terminal: int, End Date: string, End Station: string, End Terminal: int, Bike #: int, Subscriber Type: string, Zip Code: string, time: timestamp, hour: int, day: date, month: int, weekday: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(col('day').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: integer (nullable = true)\n",
      " |-- Duration: integer (nullable = true)\n",
      " |-- Start Date: string (nullable = true)\n",
      " |-- Start Station: string (nullable = true)\n",
      " |-- Start Terminal: integer (nullable = true)\n",
      " |-- End Date: string (nullable = true)\n",
      " |-- End Station: string (nullable = true)\n",
      " |-- End Terminal: integer (nullable = true)\n",
      " |-- Bike #: integer (nullable = true)\n",
      " |-- Subscriber Type: string (nullable = true)\n",
      " |-- Zip Code: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: date (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a temporary SQL table from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following exercises, where possible use both dataframe methods and SQL queries\n",
    "\n",
    "Hint: In Hive SQL, you can refer to column names including spaces with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "SELECT `column name` FROM table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353872"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  353872|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) FROM trips').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean, standard deviation, minimum and maximum of the duration column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          Duration|\n",
      "+-------+------------------+\n",
      "|  count|            353872|\n",
      "|   mean| 1043.044581091468|\n",
      "| stddev|30027.518214567655|\n",
      "|    min|                60|\n",
      "|    max|          17270400|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Duration').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+------------------+---+--------+\n",
      "| count|             mean|               std|min|     max|\n",
      "+------+-----------------+------------------+---+--------+\n",
      "|353872|1043.044581091468|30027.518214567655| 60|17270400|\n",
      "+------+-----------------+------------------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT COUNT(*) AS count, \n",
    "AVG(Duration) AS mean, \n",
    "STDDEV(Duration) AS std, \n",
    "MIN(Duration) AS min, \n",
    "MAX(Duration) AS max \n",
    "FROM trips\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For how many different days do you have observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('day').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|353872|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) AS count FROM trips').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT day)|\n",
      "+-------------------+\n",
      "|                365|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(DISTINCT(day)) FROM trips\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Duration=60)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Duration').rdd.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the first and last observed days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = df.select('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       day|\n",
      "+----------+\n",
      "|2015-08-31|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day.sort('day', ascending=False).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       day|\n",
      "+----------+\n",
      "|2014-09-01|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "day.sort('day', ascending=True).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "| first day|  last day|\n",
      "+----------+----------+\n",
      "|2014-09-01|2015-08-31|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT MIN(day) AS `first day`, MAX(day) AS `last day` FROM trips\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the counts of rides per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|   0| 1012|\n",
      "|   1|  506|\n",
      "|   2|  281|\n",
      "|   3|  156|\n",
      "|   4|  640|\n",
      "|   5| 1848|\n",
      "|   6| 8012|\n",
      "|   7|24742|\n",
      "|   8|49414|\n",
      "|   9|34929|\n",
      "|  10|15306|\n",
      "|  11|14041|\n",
      "|  12|15769|\n",
      "|  13|14652|\n",
      "|  14|12788|\n",
      "|  15|16466|\n",
      "|  16|31813|\n",
      "|  17|45798|\n",
      "|  18|30956|\n",
      "|  19|14899|\n",
      "|  20| 8245|\n",
      "|  21| 5738|\n",
      "|  22| 3658|\n",
      "|  23| 2203|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('hour').count().sort('hour').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|hour|count(1)|\n",
      "+----+--------+\n",
      "|   0|    1012|\n",
      "|   1|     506|\n",
      "|   2|     281|\n",
      "|   3|     156|\n",
      "|   4|     640|\n",
      "|   5|    1848|\n",
      "|   6|    8012|\n",
      "|   7|   24742|\n",
      "|   8|   49414|\n",
      "|   9|   34929|\n",
      "|  10|   15306|\n",
      "|  11|   14041|\n",
      "|  12|   15769|\n",
      "|  13|   14652|\n",
      "|  14|   12788|\n",
      "|  15|   16466|\n",
      "|  16|   31813|\n",
      "|  17|   45798|\n",
      "|  18|   30956|\n",
      "|  19|   14899|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT hour, COUNT(*) FROM trips \n",
    "GROUP BY hour \n",
    "ORDER BY hour\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the counts per hour averaged over all observed dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|        avg(count)|\n",
      "+----+------------------+\n",
      "|   0|3.1138461538461537|\n",
      "|   1| 2.219298245614035|\n",
      "|   2|1.7239263803680982|\n",
      "|   3|1.3333333333333333|\n",
      "|   4|2.4334600760456273|\n",
      "|   5| 6.019543973941368|\n",
      "|   6| 22.95702005730659|\n",
      "|   7| 67.97252747252747|\n",
      "|   8|135.75274725274724|\n",
      "|   9| 95.69589041095891|\n",
      "|  10| 42.04945054945055|\n",
      "|  11| 38.46849315068493|\n",
      "|  12|43.202739726027396|\n",
      "|  13| 40.14246575342466|\n",
      "|  14|35.035616438356165|\n",
      "|  15| 45.11232876712329|\n",
      "|  16| 87.15890410958905|\n",
      "|  17|125.47397260273972|\n",
      "|  18| 84.81095890410958|\n",
      "|  19| 40.81917808219178|\n",
      "|  20|22.589041095890412|\n",
      "|  21|15.807162534435262|\n",
      "|  22|10.104972375690608|\n",
      "|  23| 6.188202247191011|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('hour', 'day').count().sort('day').select('hour', 'count').groupby('hour').agg({'count': 'mean'}).sort('hour').show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|       avg(counts)|\n",
      "+----+------------------+\n",
      "|   0|3.1138461538461537|\n",
      "|   1| 2.219298245614035|\n",
      "|   2|1.7239263803680982|\n",
      "|   3|1.3333333333333333|\n",
      "|   4|2.4334600760456273|\n",
      "|   5| 6.019543973941368|\n",
      "|   6| 22.95702005730659|\n",
      "|   7| 67.97252747252747|\n",
      "|   8|135.75274725274724|\n",
      "|   9| 95.69589041095891|\n",
      "|  10| 42.04945054945055|\n",
      "|  11| 38.46849315068493|\n",
      "|  12|43.202739726027396|\n",
      "|  13| 40.14246575342466|\n",
      "|  14|35.035616438356165|\n",
      "|  15| 45.11232876712329|\n",
      "|  16| 87.15890410958905|\n",
      "|  17|125.47397260273972|\n",
      "|  18| 84.81095890410958|\n",
      "|  19| 40.81917808219178|\n",
      "|  20|22.589041095890412|\n",
      "|  21|15.807162534435262|\n",
      "|  22|10.104972375690608|\n",
      "|  23| 6.188202247191011|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT hour, AVG(counts) \n",
    "FROM (\n",
    "    SELECT hour, day, COUNT(*) AS counts \n",
    "    FROM trips group by hour, day\n",
    "    ) s \n",
    "GROUP BY hour \n",
    "ORDER BY hour\n",
    "''').show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the average duration of trips per hour departing from terminal 70 sorted by the hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|     avg(Duration)|\n",
      "+----+------------------+\n",
      "|   0|2681.2597402597403|\n",
      "|   1| 886.7142857142857|\n",
      "|   2| 809.6666666666666|\n",
      "|   3|             242.0|\n",
      "|   4|           29449.5|\n",
      "|   5|2305.2820512820513|\n",
      "|   6| 726.3856317093312|\n",
      "|   7| 750.8696909050478|\n",
      "|   8| 727.0969317661426|\n",
      "|   9| 706.4761441090556|\n",
      "|  10|  872.337684943429|\n",
      "|  11|1450.3474576271187|\n",
      "|  12| 2403.685567010309|\n",
      "|  13| 2514.535117056856|\n",
      "|  14| 1452.651567944251|\n",
      "|  15| 906.8820512820513|\n",
      "|  16| 1006.927950310559|\n",
      "|  17| 725.6462147451757|\n",
      "|  18| 728.8278822567457|\n",
      "|  19|  658.924486803519|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Start Terminal']==70).groupby('hour').agg({'Duration': 'mean'}).sort('hour').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|hour|     avg(duration)|\n",
      "+----+------------------+\n",
      "|   0|2681.2597402597403|\n",
      "|   1| 886.7142857142857|\n",
      "|   2| 809.6666666666666|\n",
      "|   3|             242.0|\n",
      "|   4|           29449.5|\n",
      "|   5|2305.2820512820513|\n",
      "|   6| 726.3856317093312|\n",
      "|   7| 750.8696909050478|\n",
      "|   8| 727.0969317661426|\n",
      "|   9| 706.4761441090556|\n",
      "|  10|  872.337684943429|\n",
      "|  11|1450.3474576271187|\n",
      "|  12| 2403.685567010309|\n",
      "|  13| 2514.535117056856|\n",
      "|  14| 1452.651567944251|\n",
      "|  15| 906.8820512820513|\n",
      "|  16| 1006.927950310559|\n",
      "|  17| 725.6462147451757|\n",
      "|  18| 728.8278822567457|\n",
      "|  19|  658.924486803519|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT hour, AVG(duration) FROM trips \n",
    "WHERE `Start Terminal`=70 \n",
    "GROUP BY hour \n",
    "ORDER BY hour\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
