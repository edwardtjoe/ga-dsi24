{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\"> \n",
    "# Spark MLlib Lab\n",
    "\n",
    "*Authors: Christoph Rahmede (LDN)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ps.SparkContext('local[4]')\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = ps.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding categorical features\n",
    "\n",
    "Often we have categorical features with values given as strings which we would like to transform to numerical values. The analogue of sklearn's `LabelEncoder` is the `StringIndexer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_1 = sqlContext.createDataFrame([\n",
    "    (4, \"high\"),\n",
    "    (5, \"low\"),\n",
    "    (6, \"high\"),\n",
    "    (7, \"high\"),\n",
    "    (8,'medium')\n",
    "], [\"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer(\n",
    "        inputCols=['label'],\n",
    "        outputCols=['label' + \"_index\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id| label|label_index|\n",
      "+---+------+-----------+\n",
      "|  4|  high|        0.0|\n",
      "|  5|   low|        1.0|\n",
      "|  6|  high|        0.0|\n",
      "|  7|  high|        0.0|\n",
      "|  8|medium|        2.0|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_2 = string_indexer.fit(ex_1).transform(ex_1)\n",
    "ex_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(\n",
    "        dropLast=True,\n",
    "        inputCols=['label_index'],\n",
    "        outputCols=['label' + \"_index_1\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+-------------+\n",
      "| id| label|label_index|label_index_1|\n",
      "+---+------+-----------+-------------+\n",
      "|  4|  high|        0.0|(2,[0],[1.0])|\n",
      "|  5|   low|        1.0|(2,[1],[1.0])|\n",
      "|  6|  high|        0.0|(2,[0],[1.0])|\n",
      "|  7|  high|        0.0|(2,[0],[1.0])|\n",
      "|  8|medium|        2.0|    (2,[],[])|\n",
      "+---+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onehot.fit(ex_2).transform(ex_2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot-encoded values are given as a sparse vector for each observation. The first number indicates the length of the sparse vector, the second number in brackets indicates the position that is filled with the last value. As you can see from the last shown entry, dropping a redundant label (`drop_last`) is default here. You can apply both `StringIndexer` and `OneHotEncoder` to multiple columns at once as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the car evaluation dataset \n",
    "\n",
    "Use `acceptability` as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv(\n",
    "    path=\"data/car.csv\",\n",
    "    header=True,\n",
    "    # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    mode=\"DROPMALFORMED\",\n",
    "    # Not always perfect but works well in most cases as of 2.1+\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(buying='vhigh', maint='vhigh', doors='2', persons='2', lug_boot='small', safety='low', acceptability='unacc')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buying', 'string'),\n",
       " ('maint', 'string'),\n",
       " ('doors', 'string'),\n",
       " ('persons', 'string'),\n",
       " ('lug_boot', 'string'),\n",
       " ('safety', 'string'),\n",
       " ('acceptability', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'acceptability']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[spark_df.dtypes[i][0] for i in range(len(spark_df.dtypes)) if spark_df.dtypes[i][1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[buying: string, maint: string, doors: string, persons: string, lug_boot: string, safety: string, acceptability: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummify the categorical variables\n",
    "\n",
    "Use first the `StringIndexer`, then the `OneHotEncoderEstimator` to create the dummified variables. Be careful not to use one-hot encoding on the target variable (`acceptability`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buying_index',\n",
       " 'maint_index',\n",
       " 'doors_index',\n",
       " 'persons_index',\n",
       " 'lug_boot_index',\n",
       " 'safety_index',\n",
       " 'acceptability_index']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col+'_index' for col in spark_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = StringIndexer(\n",
    "        inputCols = spark_df.columns,\n",
    "        outputCols = [col+'_index' for col in spark_df.columns]\n",
    "    )\n",
    "\n",
    "spark_df = string_indexer.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[buying: string, maint: string, doors: string, persons: string, lug_boot: string, safety: string, acceptability: string, lug_boot_index: double, persons_index: double, maint_index: double, safety_index: double, buying_index: double, acceptability_index: double, doors_index: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'acceptability_index'\n",
    "feature_columns = [col for col in spark_df.columns if col!=label_column and 'index' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(\n",
    "        dropLast=True,\n",
    "        inputCols=feature_columns,\n",
    "        outputCols=[col + \"_1\" for col in feature_columns]\n",
    "    )\n",
    "spark_df = onehot.fit(spark_df).transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------+--------+------+-------------+--------------+-------------+-----------+------------+------------+-------------------+-----------+--------------+-------------+----------------+---------------+--------------+-------------+\n",
      "|buying|maint|doors|persons|lug_boot|safety|acceptability|lug_boot_index|persons_index|maint_index|safety_index|buying_index|acceptability_index|doors_index|safety_index_1|maint_index_1|lug_boot_index_1|persons_index_1|buying_index_1|doors_index_1|\n",
      "+------+-----+-----+-------+--------+------+-------------+--------------+-------------+-----------+------------+------------+-------------------+-----------+--------------+-------------+----------------+---------------+--------------+-------------+\n",
      "| vhigh|vhigh|    2|      2|   small|   low|        unacc|           2.0|          0.0|        3.0|         1.0|         3.0|                0.0|        0.0| (2,[1],[1.0])|    (3,[],[])|       (2,[],[])|  (2,[0],[1.0])|     (3,[],[])|(3,[0],[1.0])|\n",
      "| vhigh|vhigh|    2|      2|   small|   med|        unacc|           2.0|          0.0|        3.0|         2.0|         3.0|                0.0|        0.0|     (2,[],[])|    (3,[],[])|       (2,[],[])|  (2,[0],[1.0])|     (3,[],[])|(3,[0],[1.0])|\n",
      "+------+-----+-----+-------+--------+------+-------------+--------------+-------------+-----------+------------+------------+-------------------+-----------+--------------+-------------+----------------+---------------+--------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your feature columns with `VectorAssembler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [col for col in spark_df.columns if '1' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['safety_index_1',\n",
       " 'maint_index_1',\n",
       " 'lug_boot_index_1',\n",
       " 'persons_index_1',\n",
       " 'buying_index_1',\n",
       " 'doors_index_1']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(buying='vhigh', maint='vhigh', doors='2', persons='2', lug_boot='small', safety='low', acceptability='unacc', lug_boot_index=2.0, persons_index=0.0, maint_index=3.0, safety_index=1.0, buying_index=3.0, acceptability_index=0.0, doors_index=0.0, safety_index_1=SparseVector(2, {1: 1.0}), maint_index_1=SparseVector(3, {}), lug_boot_index_1=SparseVector(2, {}), persons_index_1=SparseVector(2, {0: 1.0}), buying_index_1=SparseVector(3, {}), doors_index_1=SparseVector(3, {0: 1.0}), features=SparseVector(15, {1: 1.0, 7: 1.0, 12: 1.0}))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=feature_columns,\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "vector_df = vectorAssembler.transform(spark_df)\n",
    "\n",
    "vector_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(15,[1,7,12],[1.0...|\n",
      "|(15,[7,12],[1.0,1...|\n",
      "|(15,[0,7,12],[1.0...|\n",
      "|(15,[1,6,7,12],[1...|\n",
      "|(15,[6,7,12],[1.0...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_df.select('features').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate a spark decision tree model and tune with grid search\n",
    "\n",
    "Once done, try also other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(featuresCol='features',\n",
    "                           labelCol=label_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cv scores:\n",
      "[0.7817 0.7888 0.835  0.8659 0.8823 0.9131 0.9155 0.9438]\n",
      "Best model parameters:\n",
      "{'maxDepth': 10}\n",
      "\n",
      "Best model test accuracy:\n",
      "0.958\n"
     ]
    }
   ],
   "source": [
    "(data_train, data_test) = vector_df.randomSplit([0.7, 0.3], seed=1)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "                    predictionCol='prediction',\n",
    "                    labelCol=label_column,\n",
    "                    metricName='accuracy'\n",
    "                         )\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.maxDepth, range(3, 11)) \\\n",
    "    .build()\n",
    "\n",
    "# the actual gridsearch\n",
    "crossval = CrossValidator(estimator=model,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "model_fit = crossval.fit(data_train)\n",
    "\n",
    "print('Average cv scores:')\n",
    "print(np.around(np.array(model_fit.avgMetrics), 4))\n",
    "\n",
    "java_model = model_fit.bestModel._java_obj\n",
    "\n",
    "print('Best model parameters:')\n",
    "print({param.name: java_model.getOrDefault(java_model.getParam(param.name)) \n",
    "    for param in paramGrid[0]})\n",
    "print()\n",
    "#print(java_model.explainParams())\n",
    "\n",
    "predictions = model_fit.transform(data_test)\n",
    "\n",
    "print('Best model test accuracy:')\n",
    "print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
