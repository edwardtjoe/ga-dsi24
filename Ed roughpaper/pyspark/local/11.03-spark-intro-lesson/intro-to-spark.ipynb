{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Spark\n",
    "\n",
    "*Authors: Adapted and modified from Dave Yerrington (SF) by Christoph Rahmede (LDN)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Describe the advantages/disadvantages of Spark compared to Hadoop MapReduce\n",
    "- Define what an RDD is, by its properties and operations\n",
    "- Explain the difference between transformations and actions on an RDD\n",
    "- Implement the different transformations through use cases\n",
    "- Use Spark via python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Lesson Guide<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-0.1\">Learning Objectives</a></span></li></ul></li><li><span><a href=\"#What-is-Spark?\" data-toc-modified-id=\"What-is-Spark?-1\">What is Spark?</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-spark-ecosystem\" data-toc-modified-id=\"The-spark-ecosystem-1.1\">The spark ecosystem</a></span></li><li><span><a href=\"#MapReduce-vs-Spark\" data-toc-modified-id=\"MapReduce-vs-Spark-1.2\">MapReduce vs Spark</a></span></li><li><span><a href=\"#Spark-is-a-distributed-computer-framework-for-parallelized-applications-like-Hadoop.\" data-toc-modified-id=\"Spark-is-a-distributed-computer-framework-for-parallelized-applications-like-Hadoop.-1.3\">Spark is a distributed computer framework for parallelized applications like <em>Hadoop</em>.</a></span></li></ul></li><li><span><a href=\"#Resilient-Distributed-Datasets-(RDD)\" data-toc-modified-id=\"Resilient-Distributed-Datasets-(RDD)-2\">Resilient Distributed Datasets (RDD)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-is-an-API-for-handling-large-data-tranformations-like-Map-Reduce.\" data-toc-modified-id=\"Spark-is-an-API-for-handling-large-data-tranformations-like-Map-Reduce.-2.1\">Spark is an API for handling large data tranformations like <em>Map Reduce</em>.</a></span></li></ul></li><li><span><a href=\"#A-&quot;functional-programming-paradigm&quot;-and-DAGs\" data-toc-modified-id=\"A-&quot;functional-programming-paradigm&quot;-and-DAGs-3\">A \"functional programming paradigm\" and DAGs</a></span></li><li><span><a href=\"#Spark-architecture-:-from-your-coding-hands-to-the-cluster\" data-toc-modified-id=\"Spark-architecture-:-from-your-coding-hands-to-the-cluster-4\">Spark architecture : from your coding hands to the cluster</a></span></li><li><span><a href=\"#Spark-Jargon\" data-toc-modified-id=\"Spark-Jargon-5\">Spark Jargon</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-has-a-SQL-interface-into-dataframes-like-Hive\" data-toc-modified-id=\"Spark-has-a-SQL-interface-into-dataframes-like-Hive-5.1\">Spark has a SQL interface into dataframes like <em>Hive</em></a></span></li><li><span><a href=\"#Spark-has-a-machine-learning-library-similar-to--Scikit-Learn.\" data-toc-modified-id=\"Spark-has-a-machine-learning-library-similar-to--Scikit-Learn.-5.2\">Spark has a machine learning library similar to  <em>Scikit Learn</em>.</a></span></li><li><span><a href=\"#Spark-is-a-framework-for-building-a-high-volume-stream-processor\" data-toc-modified-id=\"Spark-is-a-framework-for-building-a-high-volume-stream-processor-5.3\">Spark is a framework for building a high volume stream processor</a></span></li></ul></li><li><span><a href=\"#Programming-with-Spark\" data-toc-modified-id=\"Programming-with-Spark-6\">Programming with Spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pyspark\" data-toc-modified-id=\"Pyspark-6.1\">Pyspark</a></span></li><li><span><a href=\"#Spark-submit\" data-toc-modified-id=\"Spark-submit-6.2\">Spark-submit</a></span></li></ul></li><li><span><a href=\"#Spark-UI\" data-toc-modified-id=\"Spark-UI-7\">Spark UI</a></span><ul class=\"toc-item\"><li><span><a href=\"#Spark-Jobs\" data-toc-modified-id=\"Spark-Jobs-7.1\">Spark Jobs</a></span></li><li><span><a href=\"#Job-Metrics\" data-toc-modified-id=\"Job-Metrics-7.2\">Job Metrics</a></span></li></ul></li><li><span><a href=\"#Initializing-a-SparkContext-in-Python\" data-toc-modified-id=\"Initializing-a-SparkContext-in-Python-8\">Initializing a <code>SparkContext</code> in Python</a></span></li><li><span><a href=\"#RDDs-vs-DataFrames\" data-toc-modified-id=\"RDDs-vs-DataFrames-9\">RDDs vs DataFrames</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-an-RDD-(from-files)\" data-toc-modified-id=\"Creating-an-RDD-(from-files)-9.1\">Creating an RDD (from files)</a></span></li><li><span><a href=\"#Creating-RDDs-from-local-files\" data-toc-modified-id=\"Creating-RDDs-from-local-files-9.2\">Creating RDDs from local files</a></span><ul class=\"toc-item\"><li><span><a href=\"#sc.parallelize()-:-create-an-RDD-from-a-python-array/list\" data-toc-modified-id=\"sc.parallelize()-:-create-an-RDD-from-a-python-array/list-9.2.1\"><code>sc.parallelize()</code> : create an RDD from a python array/list</a></span></li><li><span><a href=\"#sc.textFile()-:-from-a-text-file-!\" data-toc-modified-id=\"sc.textFile()-:-from-a-text-file-!-9.2.2\"><code>sc.textFile()</code> : from a text file !</a></span></li><li><span><a href=\"#sc.textFile()-:-from-a-directory-!\" data-toc-modified-id=\"sc.textFile()-:-from-a-directory-!-9.2.3\"><code>sc.textFile()</code> : from a directory !</a></span></li></ul></li></ul></li><li><span><a href=\"#Transformations-:-transforming-an-RDD-into-another\" data-toc-modified-id=\"Transformations-:-transforming-an-RDD-into-another-10\">Transformations : transforming an RDD into another</a></span></li><li><span><a href=\"#Actions-:-turning-your-RDD-into-something-else-(local-object)\" data-toc-modified-id=\"Actions-:-turning-your-RDD-into-something-else-(local-object)-11\">Actions : turning your RDD into something else (local object)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Applying-transformations-and-chaining-them\" data-toc-modified-id=\"Applying-transformations-and-chaining-them-11.1\">Applying transformations and chaining them</a></span></li><li><span><a href=\"#Example-of-chain-of-transformations\" data-toc-modified-id=\"Example-of-chain-of-transformations-11.2\">Example of chain of transformations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-the-words-using-map\" data-toc-modified-id=\"Split-the-words-using-map-11.2.1\">Split the words using <code>map</code></a></span></li><li><span><a href=\"#Split-the-words-using-flatMap\" data-toc-modified-id=\"Split-the-words-using-flatMap-11.2.2\">Split the words using <code>flatMap</code></a></span></li><li><span><a href=\"#Register-presence-of-words-through-indicator\" data-toc-modified-id=\"Register-presence-of-words-through-indicator-11.2.3\">Register presence of words through indicator</a></span></li><li><span><a href=\"#Count-words-with-reduceByKey\" data-toc-modified-id=\"Count-words-with-reduceByKey-11.2.4\">Count words with <code>reduceByKey</code></a></span></li><li><span><a href=\"#Sort-by-count-with-sortBy\" data-toc-modified-id=\"Sort-by-count-with-sortBy-11.2.5\">Sort by count with <code>sortBy</code></a></span></li><li><span><a href=\"#Do-all-in-one-go-by-chaining-steps-together\" data-toc-modified-id=\"Do-all-in-one-go-by-chaining-steps-together-11.2.6\">Do all in one go by chaining steps together</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-12\">Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Spark?\n",
    "---\n",
    "\n",
    "\n",
    "- Apache Spark is a (VERY) powerful open source in-memory framework\n",
    "    - It is also called **general purpose processing engine**\n",
    "- It was built on top of Hadoop and originally developed at UC Berkeley in 2009 with the aim of solving some of its problems\n",
    "- The engineers knew EXTREMELY well mapReduce. They learned a lot from:\n",
    "    - why it was so hard to work with\n",
    "    - what were the performance challenges\n",
    "\n",
    "They addressed them very well:\n",
    "- Spark was built around **speed, ease of use, and unified engine** (support libraries for SQL queries, streaming data, machine learning and graph processing)\n",
    "- It is the **largest open source project in data processing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spark ecosystem\n",
    "\n",
    "![](images/spark_ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce vs Spark\n",
    "\n",
    "\n",
    "**Hadoop MapReduce limits:**\n",
    "- your job has to fit the `<key, value>` paradigm\n",
    "- each job read from disk: problem with iterative algorithms (machine learning)\n",
    "- data is maintained via redundancy\n",
    "\n",
    "**How Spark answers this:**\n",
    "- Spark proposes **other processing workflows than MapReduce**\n",
    "- Highly efficient distributed operations\n",
    "- Spark runs in memory and on disk\n",
    "- Can be up to 100x faster than Hadoop MapReduce in memory, and 10x faster on disk\n",
    "- Spark keeps everything in memory when possible, uses lots of it\n",
    "- It's a set of tools for developing applications  \n",
    "- It allows parallel processing of applications in a distributed environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is a distributed computer framework for parallelized applications like _Hadoop_.\n",
    "\n",
    "_Spark can interact with Hadoop's HDFS to access large amounts of data using high volume, distributed I/O._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDD)\n",
    "\n",
    "\n",
    "- created from HDFS, S3, HBase, JSON, text, local... or transformed from another RDD\n",
    "- distributed accross the cluster, partitioned (atomic chunks of data)\n",
    "- can recover from errors (node failure, slow process)\n",
    "- traceability of each partition, can re-run the processing\n",
    "- **immutable** : you cannot *modify* an RDD in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is an API for handling large data tranformations like _Map Reduce_.\n",
    "\n",
    "_It is a data transformation and selection tool like **Pandas**.  You can develop transformations through an API that allows you to chain operations together in a modular fashion, similar to **Pandas**._\n",
    "\n",
    "![](https://snag.gy/9G4gJO.jpg)\n",
    "\n",
    "> However, Spark delivers the idea of data manipulation through the framework of **transformations** and **actions**.  These transformations and actions are performed in parallel, using many different worker nodes (which may be distributed on multiple machines).\n",
    "\n",
    "> Mapreduce is broken down into two main functions \"map\" and \"reduce\".  Spark on the other hand, operates through a set of operations determined through a **Directed Acyclic Graph** (or DAG for short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"functional programming paradigm\" and DAGs\n",
    "\n",
    "\n",
    "Spark provides many transformation functions. By programming these functions, you construct a **Directed Acyclic Graph** (DAG).\n",
    "\n",
    "<img src=\"images/dag.png\">\n",
    "\n",
    "\n",
    "When you use them, these functions are passed from the **client** to the **master**, who then distributes them to workers, who apply them across their partitions of the RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://snag.gy/ieVW98.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark architecture : from your coding hands to the cluster\n",
    "\n",
    "<img src=\"images/from_rdd_to_cluster.png\">\n",
    "\n",
    "You construct your sequence of transformations in python.\n",
    "The Spark functional programming interface builds up a **DAG**.\n",
    "This DAG is sent by the **driver** to the **cluster manager** for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Jargon\n",
    "\n",
    "Excerpt taken from [Arush Kharbanda](https://www.quora.com/What-exactly-is-Apache-Spark-and-how-does-it-work) on Quora.\n",
    "\n",
    "**Job**: A piece of code which reads some input  from HDFS or local, performs some computation on the data and writes some output data.\n",
    "\n",
    "**Stages**: Jobs are divided into stages. Stages are classified as Map or Reduce stages. Stages are divided based on computational boundaries, not all computations (operators) can be updated in a single Stage. It happens over many stages.\n",
    "\n",
    "**Tasks**: Each stage has some tasks, one task per partition. One task is executed on one partition of data on one executor (machine).\n",
    "\n",
    "**DAG**: DAG stands for Directed Acyclic Graph, in the present context it's a DAG of operators.\n",
    "\n",
    "**Executor**: The process responsible for executing a task.\n",
    "\n",
    "**Driver**: The program/process responsible for running the Job over the Spark Engine\n",
    "\n",
    "**Master**: The machine on which the Driver program runs\n",
    "\n",
    "**Worker**: The machine on which the Executor program runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark has a SQL interface into dataframes like _Hive_\n",
    "\n",
    "Spark isn't exactly **Hive**, but it uses components from Hive.  You can use temporary SQL views with Spark dataframes.\n",
    "\n",
    ">```python\n",
    "># Load a dataset as a Spark DataFrame\n",
    ">df = spark.read.csv(\"datasets/somedataset/hamburgers_eaten_per_hour.csv\")\n",
    ">df.createOrReplaceTempView(\"hamburgers\")\n",
    ">```\n",
    "\n",
    "\n",
    "\n",
    "Then you can slice and dice your dataframe with SQL:\n",
    "\n",
    ">```python\n",
    ">spark.sql(\"SELECT * FROM hamburgers\").show()\n",
    ">\n",
    "># +------+---------+\n",
    "># | eaten|     name|\n",
    "># +------+---------+\n",
    "># |null  |     Jeff|\n",
    "># |  30  |   Kiefer|\n",
    "># |  19  |     Hang|\n",
    "># +------+---------+\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark has a machine learning library similar to  _Scikit Learn_.\n",
    "\n",
    "![](https://snag.gy/RnuX6h.jpg)\n",
    "\n",
    "_Spark provides an interface to MLib via Scala, Java, Python, and R.  The most common methods are provided such as regression, support vector machines, and random forests, however, not all evaluation metrics are available in Python yet.  Spark is written in Scala, so features are prioritized to Scala first throughout the Spark ecosystem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is a framework for building a high volume stream processor\n",
    "![](https://snag.gy/RCikuU.jpg)\n",
    "\n",
    "With **Spark streaming**, it's possible to build a process that can respond to data in **real-time**, using any of Spark's features including Mlib, GraphX, or any kinds of transformations you could do within the Spark context.  The streaming capabilities of Spark core make it possible to produce real-time applications such as **ETL, analytics dashboards, data mining, or large scale aggregations**.\n",
    "\n",
    "In short, you can create a \"streaming context\" that listens on a specific port, and tie to any number of operations that can be programed with spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming with Spark\n",
    "---\n",
    "\n",
    "Running applications built with Spark has a variety of options:\n",
    "\n",
    "- Pyspark\n",
    "- spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark\n",
    "Pyspark is a real-time interpreter for Spark for Python.  You can integrate operations with the Spark Python libraries in real-time.  This is a great way to prototype applications much in the same way we do with Jupyter notebook.  It's also possible to connect Pyspark to Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark-submit\n",
    "Spark-submit, on the other hand, is a way to run a set of application instructions bundled in a single file. It's possible to write these in Python, Scala, or Java.  Typically, it's convenient to prototype a set of operations that you want performed on a dataset with Pyspark, debug them to a high level of quality, then run them with Spark-submit.\n",
    "\n",
    "> With spark-submit, it's also possible to tune the parameters in which your application will run to a very granular degree including memory, number of total cores, and specific cluster modes to control test versus production deployments.\n",
    ">\n",
    "> [Read more about submitting applications](http://spark.apache.org/docs/latest/submitting-applications.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark UI\n",
    "---\n",
    "\n",
    "Anytime a \"spark context\" is created, a corresponding spark UI is launched. It is accessible **only** while the Spark application is running. Through the web UI, you can monitor how your applications run.  Anything that the Spark context handles, even the one line operations from PySpark, can be observed as separate jobs in the Spark UI.\n",
    "\n",
    "Check: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Jobs\n",
    "![](https://snag.gy/JnuSKC.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Metrics\n",
    "![](https://snag.gy/JW2fOb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a `SparkContext` in Python\n",
    "\n",
    "IPython / Jupyter notebook can be a *client* to interact with the *master*.\n",
    "\n",
    "The client will have a `SparkContext` (\"sc\") that..\n",
    "\n",
    "1. Acts as a gateway between the client and Spark master\n",
    "2. Sends code/data from IPython to the master (who then sends it to the workers)\n",
    "\n",
    "<img src=\"https://snag.gy/iCm4G1.jpg\" width=\"600\">\n",
    "\n",
    "**\"sc\"** represents your interface to a running spark cluster manager.  A Spark context is defined as a preconfigured cluster, an application name connected to it.  All **transformations** and **actions** performed by Spark are handled through the Spark context (aka: **sc**).\n",
    "\n",
    "Using:\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "sc = ps.SparkContext('local[*]')\n",
    "```\n",
    "\n",
    "will create a *local* cluster made of the driver using all your CPU cores, the operating system addresses the allocated cores and shares the workload between them when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we try to create a SparkContext to work locally on all cpus available\n",
    "sc = ps.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to stop your spark context instance:\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: Spark does some very heavy lifting for us and works very well most of the time.\n",
    "It is not perfect though. If things start getting ugly (understand pyspark unable to create a spark context connection) run in terminal:\n",
    "\n",
    "```\n",
    "ps -ax | grep spark\n",
    "kill <PID>\n",
    "```\n",
    "\n",
    "This is quite an extreme measure. Use it only if nothing else works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs vs DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "The two main types of data objects in Spark are the **Resilient Distributed Dataset** and the **DataFrame**.  Both types represent data in a distributed state.  RDDs store data in a more primitive state such as a list of pairs, integers, floats, or strings.  DataFrames have a rich structure defintion called a **schema** much like a Pandas dataframe.\n",
    "\n",
    "- You use **RDDs** to manage semi-structured data.\n",
    "- You use **DataFrames** to operate on typed series.\n",
    "\n",
    "Both RDDs and DataFrames can contain multiple types of objects.  **DataFrames** are much more constrained because data is represented by a two-dimensional tabular structure where columns represent variables and rows observations.  **RDDs** are much more flexible if your data requires much less structure than a DataFrame while still being able to use _transformation_ methods such as **`map()`** and _action_ methods such as **`reduce()`**.\n",
    "\n",
    ">_Distributed Data in Spark_\n",
    ">![](https://snag.gy/vxVhri.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD (from files)\n",
    "\n",
    "RDDs are **immutable**. Once created, you cannot modify them directly. You can only transform them into another RDD. \n",
    "\n",
    "Functions for creating an RDD from an external source are methods of the SparkContext object `sc`.\n",
    "\n",
    "| Method | Description |\n",
    "| - | - |\n",
    "| `sc.parallelize(array)` | Create an RDD from a python array or list |\n",
    "| `sc.textFile(path)` | Create an RDD from a text file |\n",
    "| `sc.pickleFile(path)` | Create an RDD from an HDFS pickle file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs from local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.parallelize()` : create an RDD from a python array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ps.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an adhoc list\n",
    "data_array = ([['isaac', 18],\n",
    "              ['lee', 7],\n",
    "              ['brad', 2],\n",
    "              ['giovanna', 14],\n",
    "              ['darren', 10],\n",
    "              ['cary', 42]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the array/list using SparkContext\n",
    "rdd = sc.parallelize(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['isaac', 18],\n",
       " ['lee', 7],\n",
       " ['brad', 2],\n",
       " ['giovanna', 14],\n",
       " ['darren', 10],\n",
       " ['cary', 42]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the results (lazy)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop your spark session, run:\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile()` : from a text file !\n",
    "\n",
    "The import will give you an rdd made of **strings which are lines of the text file**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\r\n",
      "jorge,8\r\n",
      "josh,15\r\n",
      "evangeline,16\r\n",
      "emilie,23\r\n",
      "yunjin,42\r\n"
     ]
    }
   ],
   "source": [
    "cat 'data/toy_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew,4\n",
      "jorge,8\n",
      "josh,15\n",
      "evangeline,16\n",
      "emilie,23\n",
      "yunjin,42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc = ps.SparkContext('local[2]')\n",
    "\n",
    "# displaying the content of the file in stdout\n",
    "with open('data/toy_data.txt', 'r') as text:\n",
    "    print(text.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matthew,4', 'jorge,8', 'josh,15', 'evangeline,16', 'emilie,23', 'yunjin,42']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the file using SparkContext\n",
    "rdd = sc.textFile('data/toy_data.txt')\n",
    "\n",
    "# to output the content in python (use collect() with great care)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/toy_data.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile()` : from a directory !\n",
    "\n",
    "We can read in all text files in a directory in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184-0.txt pg11.txt   pg16.txt   pg236.txt  pg84.txt\r\n",
      "4300-0.txt pg1342.txt pg1661.txt pg2500.txt pg844.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/project_gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg's The Count of Monte Cristo, by Alexandre Dumas, père\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost\r",
      "\r\n",
      "no restrictions whatsoever.  You may copy it, give it away or re-use it\r",
      "\r\n",
      "under the terms of the Project Gutenberg License included with this\r",
      "\r\n",
      "eBook or online at www.gutenberg.org\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "Title: The Count of Monte Cristo\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head data/project_gutenberg/1184-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('data/project_gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we won't be able to look at the rdd-content in one go. Applying `.collect()` could be dangerous. Instead we can read out the first or first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Project Gutenberg's Frankenstein, by Mary Wollstonecraft (Godwin) Shelley\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Project Gutenberg's Frankenstein, by Mary Wollstonecraft (Godwin) Shelley\",\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations : transforming an RDD into another\n",
    "\n",
    "- They are **lazy**: Spark doesn't apply the transformation right away, it just builds on the **DAG**.\n",
    "- They transform an RDD into another RDD because RDDs are **immutable**.\n",
    "- They can be **wide** or **narrow** (whether they shuffle partitions or not).\n",
    "\n",
    "<img src=\"images/rdd_narrow_vs_wide_transformations.png\" width=\"400\"/>\n",
    "\\[[Image Source](http://horicky.blogspot.com/2013/12/spark-low-latency-massively-parallel.html)\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Type | Category | Description |\n",
    "| - | - | - |-|\n",
    "| [`.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) | transformation | mapping | Return a new RDD by applying a function to each element of this RDD. |\n",
    "| [`.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap) | transformation | mapping | Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. |\n",
    "| [`.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter) | transformation | reduction |  Return a new RDD containing only the elements that satisfy a predicate. |\n",
    "| [`.sample()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample) | transformation | reduction | Return a sampled subset of this RDD. |\n",
    "| [`.distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.distinct) | transformation | reduction |  Return a new RDD containing the distinct elements in this RDD. |\n",
    "| [`.keys()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.keys) | transformation | `<k,v>` | Return an RDD with the keys of each tuple. |\n",
    "| [`.values()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.values) | transformation | `<k,v>` | Return an RDD with the values of each tuple. |\n",
    "| [`.join(rddB)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join) | transformation | `<k,v>` | Return an RDD containing all pairs of elements with matching keys in self and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other. |\n",
    "| [`.reduceByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) | transformation | `<k,v>` | Merge the values for each key using an associative and commutative reduce function. |\n",
    "| [`.groupByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) | transformation | `<k,v>` | Group the values for each key in the RDD into a single sequence. |\n",
    "| [`.sortBy(keyfunc)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy) | transformation | sorting |  Sorts this RDD by the given keyfunc. |\n",
    "| [`.sortByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortByKey) | transformation | sorting/`<k,v>` | Sorts this RDD which is assumed to consist of (key, value) pairs. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions : turning your RDD into something else (local object)\n",
    "\n",
    "Actions are specific methods of an RDD object, they are usually designed to transform an RDD into something else (e.g. a python object or a statistic).\n",
    "\n",
    "When used/executed in IPython or in a notebook, they **launch the processing of the DAG**. This is where Spark stops being **lazy**. This is where your script will take time to execute.\n",
    "\n",
    "| Method | Type | Description |\n",
    "| - | - | - |\n",
    "|[`.reduce()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduce)| action | Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.|\n",
    "| [`.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) | action | Return a list that contains all of the elements in this RDD. Note that this method should only be used if the resulting array is expected to be small as all the data is loaded into the driver’s memory. |\n",
    "| [`.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) | action | Return the number of elements in this RDD. |\n",
    "| [`.take(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) | action | Take the first `n` elements of the RDD. |\n",
    "| [`.top(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top) | action | Get the top `n` elements from a RDD. It returns the list sorted in descending order. |\n",
    "| [`.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first) | action | Return the first element in an RDD. |\n",
    "| [`.sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sum) | action | Add up the elements in this RDD. |\n",
    "| [`.mean()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mean) | action | Compute the mean of this RDD’s elements. |\n",
    "| [`.stdev()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.stdev) | action | Compute the standard deviation of this RDD’s elements. |\n",
    "| [`.countByKey()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)|action| Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying transformations and chaining them\n",
    "\n",
    "Recall the spark flow:\n",
    "\n",
    "<img src=\"images/spark_flow.png\" width=\"500\">\n",
    "\n",
    "We'll proceed along the usual spark flow (see above):\n",
    "1. Create the environment to run spark from python\n",
    "2. Extract RDDs from files\n",
    "3. Run some transformations\n",
    "4. Execute actions to obtain values (local objects in python)\n",
    "\n",
    "Each transformation is a method of an RDD and returns another RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of chain of transformations\n",
    "\n",
    "We could try to do the wordcount from the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the words using `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Project',\n",
       "  \"Gutenberg's\",\n",
       "  'Frankenstein,',\n",
       "  'by',\n",
       "  'Mary',\n",
       "  'Wollstonecraft',\n",
       "  '(Godwin)',\n",
       "  'Shelley'],\n",
       " [],\n",
       " ['This',\n",
       "  'eBook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with'],\n",
       " ['almost',\n",
       "  'no',\n",
       "  'restrictions',\n",
       "  'whatsoever.',\n",
       "  'You',\n",
       "  'may',\n",
       "  'copy',\n",
       "  'it,',\n",
       "  'give',\n",
       "  'it',\n",
       "  'away',\n",
       "  'or'],\n",
       " ['re-use',\n",
       "  'it',\n",
       "  'under',\n",
       "  'the',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Project',\n",
       "  'Gutenberg',\n",
       "  'License',\n",
       "  'included']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x.split()).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the words using `flatMap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " \"Gutenberg's\",\n",
       " 'Frankenstein,',\n",
       " 'by',\n",
       " 'Mary',\n",
       " 'Wollstonecraft',\n",
       " '(Godwin)',\n",
       " 'Shelley',\n",
       " 'This',\n",
       " 'eBook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'at',\n",
       " 'no',\n",
       " 'cost']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_flat = rdd.flatMap(lambda x: x.split())\n",
    "words_flat.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register presence of words through indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 1),\n",
       " (\"Gutenberg's\", 1),\n",
       " ('Frankenstein,', 1),\n",
       " ('by', 1),\n",
       " ('Mary', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_registered = words_flat.map(lambda x: (x, 1)) \n",
    "words_registered.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count words with `reduceByKey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 74),\n",
       " ('Wollstonecraft', 4),\n",
       " ('is', 8796),\n",
       " ('of', 34367),\n",
       " ('at', 6979)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_reduced = words_registered.reduceByKey(lambda x, y: x+y ) \n",
    "words_reduced.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort by count with `sortBy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 63656),\n",
       " ('of', 34367),\n",
       " ('and', 32787),\n",
       " ('to', 31399),\n",
       " ('a', 24811),\n",
       " ('in', 18168),\n",
       " ('I', 18070),\n",
       " ('his', 13485),\n",
       " ('he', 13299),\n",
       " ('was', 13029)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_sorted = words_reduced.sortBy(lambda x: x[1], ascending=False) \n",
    "words_sorted.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do all in one go by chaining steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 63656),\n",
       " ('of', 34367),\n",
       " ('and', 32787),\n",
       " ('to', 31399),\n",
       " ('a', 24811),\n",
       " ('in', 18168),\n",
       " ('I', 18070),\n",
       " ('his', 13485),\n",
       " ('he', 13299),\n",
       " ('was', 13029)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x+y ) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_result = rdd.flatMap(lambda x: x.split()) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x+y ) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 63656),\n",
       " ('of', 34367),\n",
       " ('and', 32787),\n",
       " ('to', 31399),\n",
       " ('a', 24811),\n",
       " ('in', 18168),\n",
       " ('I', 18070),\n",
       " ('his', 13485),\n",
       " ('he', 13299),\n",
       " ('was', 13029)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(words_result.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Spark is one of the essential big data tools. While we use it only on our laptops for now, spark jobs can easily be run on a cluster. We will practice the tools learned in this lesson more and are going to see how to use the machine learning library of spark."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lesson Guide",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "579px",
    "left": "561.111px",
    "right": "20px",
    "top": "120px",
    "width": "337px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
